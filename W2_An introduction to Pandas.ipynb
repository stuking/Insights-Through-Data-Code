{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed66a387",
   "metadata": {},
   "source": [
    "# An introduction to Pandas\n",
    "\n",
    "[pandas](http://pandas.pydata.org) is a module which allows the construction of a *dataframe*, this is an object to store data that looks a little like a spreadsheet (the data is indexed principally by a column name and row name/number). It also includes functions designed to make working with this structured or tabular data fast and easy.  \n",
    "\n",
    "Again, the website for Pandas is good and contains the main set of documentation. It is a harder module to understand, and the documentation on the website is more dense to read. [Here](http://pandas.pydata.org/index.html) is the main website. \n",
    "- The main documentation for Pandas is [here](http://pandas.pydata.org/pandas-docs/stable/).\n",
    "- There is a quick introduction to Pandas [here](http://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "- There is a fantastic tutorial (also in Jupyter) [here](http://pandas.pydata.org/pandas-docs/stable/tutorials.html) (under Lessons for New Pandas Users). This is well worth working through a little if you want a longer introduction to the basic concepts in Pandas.\n",
    "\n",
    "Here are some basic examples to getting started with pandas, the datasets we use need to be copied into the same location as this notebook in order to use them in our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3702dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pandas import statement\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8c314",
   "metadata": {},
   "source": [
    "Next we will look at a dataset containing some information about recent oil reserves possessed by several different countries. Pandas allows us to *read* this data into a dataframe and then to extract various aspects of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd20140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the read_csv method to make a dataframe from \n",
    "# the csv file:\n",
    "A = pd.read_csv('oil_reserve_data.csv')\n",
    "# We can look at what the dataframe contains \n",
    "# note it looks a little like a spreadsheet view of the data\n",
    "print(A)\n",
    "print('-----')\n",
    "# The keys() are the various names indexing our data - we can pull the \n",
    "# different columns by using these names:\n",
    "print(A.keys())\n",
    "print('-----')\n",
    "# Let's pull the data from a particular column by referring to it\n",
    "# by name:\n",
    "print(A['Germany'])\n",
    "print('-----')\n",
    "# We can also slice a row from our data - here we take the second row by number \n",
    "# (numerical index starts counting from 0):\n",
    "print(A.iloc[1])\n",
    "print('-----')\n",
    "# Note this returns the value indexed by each column name.\n",
    "# We can also grab a column by number - here is the second column:\n",
    "print(A.iloc[:,1])\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4c4f0",
   "metadata": {},
   "source": [
    "## Removing columns/indices <a name=\"removing\"></a>\n",
    "Similar to above, it is easy to remove entries. This is done with the `drop()` method and can be applied to both columns and indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define new DataFrame using a function from NumPy (don't worry about what this line does just now)\n",
    "data = np.reshape(np.arange(9), (3,3))\n",
    "\n",
    "df = pd.DataFrame(data, index=['a','b','c'],\n",
    "                  columns=['Edinburgh', 'Glasgow', 'Dundee'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ce56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('b')  # remove row (index)~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also drop from a column - here is the way we would drop two columns:\n",
    "df.drop(['Dundee', 'Glasgow'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae355d1",
   "metadata": {},
   "source": [
    "Note that the original data frame is unchanged: `df.drop()` gives us a new data frame with the desired data dropped, and leaves the original data intact. We can ask `.drop()` to operate directly on the original data frame by setting the argument `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.drop('a',inplace=True)\n",
    "print('-----')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167ae3b",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Rows and columns in a dataframe can be referred to by name (if they are given a name - for the example at the beginning using oil reserves we didn't specify row names), or by row/column number. \n",
    "\n",
    "Important - python starts counting from 0, i.e. 0, 1, 2, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aabd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, index=['a','b','c'],\n",
    "                  columns=['Edinburgh', 'Glasgow', 'Dundee'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae09c43",
   "metadata": {},
   "source": [
    "We can get the 'a' row by name or by knowing that it is row 0 using the following two pandas functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060de81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note where the square brackets are!\n",
    "print(df.loc['a'])\n",
    "print('-----')\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8b46f",
   "metadata": {},
   "source": [
    "We can also grab two rows at a time by listing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0138b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again note where the square brackets are!\n",
    "print(df.loc[['a','b']])\n",
    "print('-----')\n",
    "print(df.iloc[[0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a96496",
   "metadata": {},
   "source": [
    "Likewise we can do similar things with column names and numbers and multiple column names and numbers - here all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Edinburgh'])\n",
    "print('-----')\n",
    "print(df.iloc[:,0])\n",
    "print('=====')\n",
    "print(df[['Edinburgh','Dundee']])\n",
    "print('-----')\n",
    "print(df.iloc[:,[0,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aea787",
   "metadata": {},
   "source": [
    "## Summarizing and computing descriptive stats\n",
    "`pandas` is equipped with common mathematical and statistical methods. Most of which fall into the category of reductions or summary statistics. These are functions that extract a single value from a list of values. For example, you can extract the sum of a column or row like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f458146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "\n",
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d94354",
   "metadata": {},
   "source": [
    "Notice how that created the sum of each column?\n",
    "\n",
    "We can sum across rows by adding an extra option to `sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02ff18",
   "metadata": {},
   "source": [
    "A similar method also exists for obtaining the mean of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e99d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e0162",
   "metadata": {},
   "source": [
    "It is also possible to get an overall statistical description of the dataframe by using the `describe()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92411a",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Clearly one thing we will need to do often is to access datasets and load them into python in order to work with them. A pandas dataframe is a really convenient way of storing data within python and manipulating it. Thankfully pandas also has functions which help load in various different standard data storage formats. \n",
    "\n",
    "We'll come back to this topic later - but for now we will just introduce two ways to load data, contained in csv and excel formats.\n",
    "\n",
    "`.csv` is a very common and portable data format. It is an easy to read file format which is usually visualised like a spreadsheet. The data itself is usually separated with a `,` which is called the **delimiter** separating enrties between the items to appear in each column, the next row is then simply on the next line.\n",
    "\n",
    "Let's now see how we can load this data and analyse it - we have already seen this line before above - but here it is again reading in a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reserves = pd.read_csv('oil_reserve_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e96cd3",
   "metadata": {},
   "source": [
    "For reading in an excel file there is a similar `read_excel()` function we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9570c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe = pd.read_excel('anscombe.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47235c6e",
   "metadata": {},
   "source": [
    "The `read_csv` function has a lot of optional arguments (more than 50). It's impossible to memorise all of them - it's usually best just to look up the particular functionality when you need it. \n",
    "\n",
    "You can search `pandas read_csv` online and find all of the documentation.\n",
    "\n",
    "There are also many other functions that can read textual data. Here are some of them:\n",
    "\n",
    "| Function | Description\n",
    "| -- | -- |\n",
    "| read_csv       | Load delimited data from a file, URL, or file-like object. The default delimiter is a comma `,` |\n",
    "| read_table     | Load delimited data from a file, URL, or file-like object. The default delimiter is tab `\\t` |\n",
    "| read_clipboard | Reads the last object you have copied (Ctrl-C) |\n",
    "| read_excel     | Read tabular data from Excel XLS or XLSX file |\n",
    "| read_hdf       | Read HDF5 file written by pandas |\n",
    "| read_html      | Read all tables found in the given HTML document |\n",
    "| read_json      | Read data from a JSON string representation |\n",
    "| read_sql       | Read the results of a SQL query |\n",
    "\n",
    "*Note: there are also other loading functions which are not touched upon here*\n",
    "\n",
    "We will talk about some of these in a later notebook to give you some examples of different data types and how to work with them and read them into a dataframe for manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fba4a1",
   "metadata": {},
   "source": [
    "## Data Cleaning \n",
    "\n",
    "While planning and constructing a data analysis, a significant amount of time is spent on data preparation: loading, cleaning, transforming and rearranging. Often the way the data is stored in files isn't in the correct format and needs to be modified. Researchers usually do this on an ad-hoc basis using programming languages like Python, we will also be talking about examples of this where we simply do some of this preparation within Excel.\n",
    "\n",
    "Here we will discuss some of the pandas tools available for handling missing data, duplicate data, string manipulation, and some other analytical data transformations.\n",
    "\n",
    "### Handling missing data \n",
    "Missing data occurs commonly in many data analysis applications. One of the goals of pandas is to make working with missing data as painless as possible.\n",
    "\n",
    "In pandas, missing numeric data is represented by `NaN` (Not a Number) and can easily be handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our example dataframe again:\n",
    "df = pd.DataFrame(data, index=['a','b','c'],\n",
    "                  columns=['Edinburgh', 'Glasgow', 'Dundee'])\n",
    "print(df)\n",
    "print('-----')\n",
    "# We will reset one element as NaN (note we have to get that from NumPy)\n",
    "df['Edinburgh']['a']=np.nan\n",
    "print(df)\n",
    "print('-----')\n",
    "# We can then find all the NaN values:\n",
    "print(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938190f",
   "metadata": {},
   "source": [
    "Here are some other methods for working with `NaN` which you can find useful:\n",
    "    \n",
    "| Method | Description |\n",
    "| -- | -- |\n",
    "| dropna | Filter axis labels based on whether the values of each label have missing data|\n",
    "| fillna | Fill in missing data with some value |\n",
    "| isnull | Return boolean values indicating which values are missing |\n",
    "| notnull | Negation of isnull |\n",
    "\n",
    "Just like `.drop()`, these methods all return a new object, leaving the original unchanged (this behaviour can be overridden using the argument `inplace=True`).\n",
    "\n",
    "`dropna()` by default removes any row that has any missing value. Here's the example with this applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e613464",
   "metadata": {},
   "source": [
    "We can also drop a row only if all of the entries are `NaN` - here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Edinburgh'] = np.nan\n",
    "df.iloc[0] = np.nan\n",
    "print(df)\n",
    "print('-----')\n",
    "print(df.dropna())\n",
    "print('-----')\n",
    "print(df.dropna(how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9e4d9",
   "metadata": {},
   "source": [
    "We could also choose to fill in the `NaN` values with a 0 say using `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b83614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "print('-----')\n",
    "print(df.fillna(0))\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb1e88",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "### Removing duplicates\n",
    "Duplicate data can be a serious issue, luckily pandas offers a simple way to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a50b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_df = pd.DataFrame(['a','b','c','d','c','b','a'])\n",
    "print(repeat_df)\n",
    "print('-----')\n",
    "print(repeat_df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3bbd5",
   "metadata": {},
   "source": [
    "You can also select which rows to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repeat_df.drop_duplicates(keep='last'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4107d6",
   "metadata": {},
   "source": [
    "### Replacing data\n",
    "We've seen how you can fill in missing non numerical data with `fillna()`. That is actually a special case of more general value replacement. That is done via the `replace()` method.\n",
    "\n",
    "Let's consider an example where the dataset given to us had `-999` as sentinel values for missing data instead of `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_df = pd.DataFrame([1., -999., 2., -999., 3., 4., -999, -999, 7.])\n",
    "print(sentinel_df)\n",
    "print('-----')\n",
    "print(sentinel_df.replace(-999, np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890c2a8",
   "metadata": {},
   "source": [
    "We could likewise have replaced with another number, or replaced text instead of numbers, and of course we can always make these changes with `inplace=True` as an option to modify our original dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1437aa",
   "metadata": {},
   "source": [
    "### Detecting and Filtering Outliers\n",
    "Filtering or transforming outliers is largely a matter of applying array operations. Consider a dataframe with some normally distributed data (don't worry about how we are generating this - but it is done with NumPy again!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb64a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two columns of normal (Gaussian, bell-shaped) data with the same parameters) \n",
    "# each with 1000 data points\n",
    "norm_data = pd.DataFrame(np.random.randn(1000, 2))\n",
    "print(norm_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f90b8a",
   "metadata": {},
   "source": [
    "Suppose we now want to find all the values exceeding 1.5 from the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791022c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_data[norm_data > 1.5])\n",
    "# to see it better remove NaN:\n",
    "print(norm_data[norm_data > 1.5].dropna(how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b51a2",
   "metadata": {},
   "source": [
    "We could set a ceiling for these values at 1.5, by simply replacing them if they exceed that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data[norm_data > 1.5] = 1.5\n",
    "norm_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82885cb9",
   "metadata": {},
   "source": [
    "Note the max of each column is now pinned at 1.5 exactly.\n",
    "\n",
    "What we just did is called **boolean indexing**. Where the condition we specify is `True` we ask pandas to do something, where it is `False` we leave the data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be08c4",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1) Read in the oil reserve data again and try extracting elements of it - \n",
    "- Get the second row\n",
    "- Get the third column\n",
    "- Get the column for 'Belgium'\n",
    "- Get a short statistical summary of all the data\n",
    "- Extract the column for 'Germany' and transform it so that the minimum value is pinned as 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e40f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a998cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44928de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eee91ede",
   "metadata": {},
   "source": [
    "2) Read in the excel data contained in 'UK_wood_imports.xlsx'. This is a UK dataset from [here](https://data.gov.uk/dataset/34758844-5e83-450a-81c9-7a0e2ca0d6b1/uk-wood-production-and-trade-2017-provisional-figures) containing information about UK wood imports (note the numbers are in thousands of cubic metres). \n",
    "- Read in the data into a dataframe and print it to the screen\n",
    "- Get a short statistical summary of the data\n",
    "- Get the third row in the 'Wood Pellets' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7261f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a07948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed787a14",
   "metadata": {},
   "source": [
    "3) Lastly read in the 'mortality.xlsx' dataset. It contains reported mortality rates from a number of countries. Missing data in this dataset is noted with an entry that is a string consisting of two full stops '..'.\n",
    "- Read in the dataset \n",
    "- Use replace to make the missing data `np.nan` rather than '..'\n",
    "- Now make a version of the dataframe where rows that have missing data (now changed to be `np.nan`) are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55208992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae0437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e419a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
